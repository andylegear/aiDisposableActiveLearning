# AI-Generated Disposable Software as Active Learning Instruments in CS Education

## Reproducibility Data Pack

This repository is the **complete data pack** for reproducing and verifying the study *"AI-Generated Disposable Software as Active Learning Instruments in CS Education"*. It contains every artifact, dataset, evaluation instrument, analysis script, and development log produced during the research â€” from initial prompt engineering through to final DSQI scoring.

**Author:** Andrew Le Gear  
**Affiliation:** ABCD Research Group, Lero, Department of Computer Science and Information Systems, University of Limerick  
**Context:** Postgraduate Diploma in Learning, Teaching and Assessment in Higher Education â€” *Leadership in Learning, Teaching and Assessment* module project  
**Date:** February 2026  
**License:** MIT (code), CC BY 4.0 (data and documentation)

---

## Study Summary

Educational software rots. Tools built for one semester break the next, and even when they survive, teachers often reject them anyway (Levy & Ben-Ari, 2007). The conventional response is to build *more durable* software. This study asks the opposite question: **what if the software is designed to be thrown away?**

We invert Wiley's (2013) disposable-versus-renewable dichotomy â€” normally applied to student assignments â€” and apply it to the *tools educators use*. The instrument is ephemeral; the learning it produces is not. Using large language models as a pedagogical forge, an educator can generate a bespoke, single-concept, browser-based active learning tool through prompt engineering alone â€” no code written by hand â€” and discard it when the class ends. Software rot becomes irrelevant when ephemerality is the design intent.

Five such artifacts were produced across five CS modules in 199 minutes of wall-clock time (6,993 lines, zero human-authored). Each is evaluated using a novel composite metric, the **Disposable Software Quality Index (DSQI)**, which combines automated static analysis (maintenance cost, creation cost) with structured human judgement (pedagogical utility via the ICAP framework, expert purity review). The goal: determine whether AI-generated throwaway code can reliably reach the *Constructive* or *Interactive* engagement modes that the active learning literature shows produce the deepest learning (Chi & Wylie, 2014).

### Key Figures

| Metric | Value |
|:--|:--|
| Artifacts produced | 5 |
| Total lines of code | 6,993 |
| Total development time | 199 minutes (~3.3 hours) |
| Lines written by human | 0 |
| AI generation ratio | 1.0 |

---

## Repository Structure

```
aiDisposableActiveLearning/
â”‚
â”œâ”€â”€ README.md                          â† You are here (data pack overview)
â”œâ”€â”€ index.html                         â† Study portal (GitHub Pages landing page)
â”œâ”€â”€ favicon.svg                        â† Shared favicon
â”‚
â”œâ”€â”€ artifacts/                         â† The 5 educational software artifacts
â”‚   â”œâ”€â”€ 01-unit-testing-gauntlet/      â† CS5703: Software Quality
â”‚   â”‚   â”œâ”€â”€ src/                       â† Deployable source (index.html, style.css, game.js)
â”‚   â”‚   â””â”€â”€ README.md                  â† Design rationale, mechanics, DSQI metrics
â”‚   â”œâ”€â”€ 02-big-o-visualiser/           â† CS4115: Data Structures and Algorithms
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ 03-sql-injection-simulator/    â† CS4416: Database Systems
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ 04-css-flexbox-trainer/        â† CS4141/CS4222: Intro to Programming / Software Dev
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ 05-lexical-analyser-trainer/   â† CS4158: Programming Language Technology
â”‚       â”œâ”€â”€ src/
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/                              â† All collected and computed data
â”‚   â”œâ”€â”€ artifact-registry.json         â† Master registry: metadata, URLs, DSQI scores
â”‚   â”œâ”€â”€ development-logs/              â† Per-artifact session logs (YAML) + WakaTime exports
â”‚   â”œâ”€â”€ evaluations/
â”‚   â”‚   â””â”€â”€ layer1-dsqi/               â† DSQI evaluation JSON per artifact
â”‚   â””â”€â”€ static-analysis/               â† Per-artifact cloc + complexity output
â”‚       â”œâ”€â”€ 01-unit-testing-gauntlet/
â”‚       â”œâ”€â”€ 02-big-o-visualiser/
â”‚       â”œâ”€â”€ 03-sql-injection-simulator/
â”‚       â”œâ”€â”€ 04-css-flexbox-trainer/
â”‚       â””â”€â”€ 05-lexical-analyser-trainer/
â”‚
â”œâ”€â”€ surveys/                           â† Evaluation instruments (deployable web tools)
â”‚   â”œâ”€â”€ expert-review-tool/            â† Expert reviewer survey (all 5 artifacts, ~25 min)
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ style.css
â”‚   â”‚   â”œâ”€â”€ survey.js
â”‚   â”‚   â””â”€â”€ survey-data.js
â”‚   â”œâ”€â”€ coordinator-review-tool/       â† Module coordinator survey (single artifact, ~10 min)
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ style.css
â”‚   â”‚   â”œâ”€â”€ survey.js
â”‚   â”‚   â””â”€â”€ survey-data.js
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ analysis/                          â† Reproducible analysis scripts
â”‚   â”œâ”€â”€ dsqi_collect.py                â† Computes DSQI M and C sub-scores from static analysis
â”‚   â””â”€â”€ wakatime_export.py             â† Exports WakaTime session data
â”‚
â”œâ”€â”€ paper/                             â† Manuscript drafts and submission materials
â”œâ”€â”€ presentation/                      â† Dissemination materials (slides, demos, handouts)
â”œâ”€â”€ checklists/                        â† Study checklists and tracking
â”‚
â”œâ”€â”€ requirements.txt                   â† Python dependencies
â””â”€â”€ .gitignore
```

---

## Artifacts

| # | Name | Module | Domain | Lines | Time | DSQI Partial |
|:-:|:-----|:-------|:-------|------:|-----:|:-------------|
| 1 | [The Unit Testing Gauntlet](artifacts/01-unit-testing-gauntlet/) | CS5703 | TDD / Red-Green-Refactor | 1,493 | 19 min | 0.422 |
| 2 | [The Big-O Dojo](artifacts/02-big-o-visualiser/) | CS4115 | Algorithm Complexity | 1,946 | 36 min | 0.436 |
| 3 | [SQL Injection Lab](artifacts/03-sql-injection-simulator/) | CS4416 | SQL Injection / Defence | 1,631 | 53 min | 0.413 |
| 4 | [Flexbox Forge](artifacts/04-css-flexbox-trainer/) | CS4141/CS4222 | CSS Flexbox Layout | 871 | 37 min | 0.440 |
| 5 | [Lex Arena](artifacts/05-lexical-analyser-trainer/) | CS4158 | Lexical Analysis / Regex | 1,052 | 54 min | 0.421 |

### Live Demos

All artifacts are deployed on GitHub Pages:

- [01 â€” The Unit Testing Gauntlet](https://andylegear.github.io/aiDisposableActiveLearning/artifacts/01-unit-testing-gauntlet/src/index.html)
- [02 â€” The Big-O Dojo](https://andylegear.github.io/aiDisposableActiveLearning/artifacts/02-big-o-visualiser/src/index.html)
- [03 â€” SQL Injection Lab](https://andylegear.github.io/aiDisposableActiveLearning/artifacts/03-sql-injection-simulator/src/index.html)
- [04 â€” Flexbox Forge](https://andylegear.github.io/aiDisposableActiveLearning/artifacts/04-css-flexbox-trainer/src/index.html)
- [05 â€” Lex Arena](https://andylegear.github.io/aiDisposableActiveLearning/artifacts/05-lexical-analyser-trainer/src/index.html)

---

## DSQI â€” Disposable Software Quality Index

The DSQI is a composite metric defined as:

```
DSQI = 0.3(1 âˆ’ M) + 0.2(1 âˆ’ C) + 0.3P + 0.2E
```

| Component | Weight | Source | Status |
|:----------|:------:|:-------|:-------|
| **M** â€” Maintenance Cost | 0.3 | Automated (cyclomatic complexity, lines of code) | âœ… Collected |
| **C** â€” Creation Cost | 0.2 | Automated (dev time, prompt count, WakaTime) | âœ… Collected |
| **P** â€” Pedagogical Utility | 0.3 | Module coordinator survey (Pâ‚ alignment + Pâ‚‚ ICAP) | ğŸ”„ In progress |
| **E** â€” Expert Purity | 0.2 | Expert reviewer survey (heuristics, fidelity, replicability) | ğŸ”„ In progress |

### Automated Results (M and C)

| Artifact | M | 1âˆ’M | C | 1âˆ’C | Partial DSQI |
|:---------|:---:|:---:|:---:|:---:|:-------------|
| 01 â€” Unit Testing Gauntlet | 0.190 | 0.810 | 0.106 | 0.894 | 0.422 + 0.3P + 0.2E |
| 02 â€” Big-O Dojo | 0.126 | 0.874 | 0.132 | 0.868 | 0.436 + 0.3P + 0.2E |
| 03 â€” SQL Injection Lab | 0.202 | 0.798 | 0.134 | 0.866 | 0.413 + 0.3P + 0.2E |
| 04 â€” Flexbox Forge | 0.143 | 0.857 | 0.084 | 0.916 | 0.440 + 0.3P + 0.2E |
| 05 â€” Lex Arena | 0.198 | 0.802 | 0.098 | 0.902 | 0.421 + 0.3P + 0.2E |

---

## Evaluation Instruments

| Instrument | Target | Duration | URL |
|:-----------|:-------|:---------|:----|
| [Expert Review Survey](surveys/expert-review-tool/) | Software engineers, CS educators | ~25 min | [Launch](https://andylegear.github.io/aiDisposableActiveLearning/surveys/expert-review-tool/index.html) |
| [Coordinator Review Survey](surveys/coordinator-review-tool/) | Module coordinators (1 artifact each) | ~10 min | [Launch](https://andylegear.github.io/aiDisposableActiveLearning/surveys/coordinator-review-tool/index.html) |

Both surveys are pure client-side web applications. Responses are downloaded as structured JSON files â€” no data is transmitted to any server.

---

## Reproducing the Analysis

### Prerequisites

- Python 3.10+
- `pip install -r requirements.txt`

### Run DSQI Collection

```bash
python analysis/dsqi_collect.py --artifact 01-unit-testing-gauntlet
python analysis/dsqi_collect.py --artifact 02-big-o-visualiser
python analysis/dsqi_collect.py --artifact 03-sql-injection-simulator
python analysis/dsqi_collect.py --artifact 04-css-flexbox-trainer
python analysis/dsqi_collect.py --artifact 05-lexical-analyser-trainer
```

---

## Development Timeline

| Date | Session | Duration | Artifacts |
|:-----|:--------|:---------|:----------|
| 2026-02-17 | Session 1 | 19 min | 01 â€” Unit Testing Gauntlet |
| 2026-02-17 | Session 2 | 36 min | 02 â€” Big-O Dojo |
| 2026-02-18 | Session 3 | 53 min | 03 â€” SQL Injection Lab |
| 2026-02-18 | Session 4 | 37 min | 04 â€” Flexbox Forge |
| 2026-02-18 | Session 5 | 54 min | 05 â€” Lex Arena |
| | **Total** | **199 min** | **5 artifacts** |

---

## Citation

```bibtex
@misc{legear2026disposable,
  author       = {Le Gear, Andrew},
  title        = {AI-Generated Disposable Software as Active Learning
                  Instruments in CS Education: Reproducibility Data Pack},
  year         = {2026},
  publisher    = {GitHub},
  url          = {https://github.com/andylegear/aiDisposableActiveLearning}
}
```

---

## Contact

**Andrew Le Gear**  
ABCD Research Group, Lero  
Department of Computer Science and Information Systems  
University of Limerick  
andrew.p.legear@ul.ie
